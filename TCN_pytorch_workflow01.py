# ##############################################################################
# @title ì…€ 1 (ìˆ˜ì •ë¨): ê¸°ë³¸ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ##############################################################################
import os
import glob
import pandas as pd
import numpy as np
from collections import Counter

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
print("-" * 50)

# ë³´ê³ ì„œì—ì„œ ê¶Œì¥í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì„¤ì •í•©ë‹ˆë‹¤.
class Config:
    # --- ë°ì´í„° ê´€ë ¨ ---
    SEQUENCE_LENGTH = 20

    # --- í›ˆë ¨ ê´€ë ¨ (ë³´ê³ ì„œ í‘œ 4 ì°¸ì¡°) ---
    OPTIMIZER = 'AdamW'
    LEARNING_RATE = 1e-4
    BATCH_SIZE = 64

    # [ìˆ˜ì • ì‚¬í•­] ê¸°ì¤€ ëª¨ë¸ê³¼ ìµœì¢… ëª¨ë¸ì˜ ì—í¬í¬ ìˆ˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì •ì˜
    # ê¸°ì¤€ ëª¨ë¸ì€ ê°€ì´ë“œ ì—­í• ë§Œ í•˜ë¯€ë¡œ ì ë‹¹íˆ í›ˆë ¨
    BASELINE_EPOCHS = 20
    # ìµœì¢… ëª¨ë¸ì€ ìµœê³  ì„±ëŠ¥ì„ ì°¾ê¸° ìœ„í•´ ë” ë§ì´ í›ˆë ¨
    FINAL_EPOCHS = 50

    # --- ëª¨ë¸ ì•„í‚¤í…ì²˜ ê´€ë ¨ (TCN, ë³´ê³ ì„œ í‘œ 4 ì°¸ì¡°) ---
    TCN_INPUT_CHANNELS = -1
    TCN_NUM_CHANNELS = [128] * 2
    TCN_KERNEL_SIZE = 3
    DROPOUT = 0.2

# ì„¤ì •ê°’ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
config = Config()

# ##############################################################################
# @title ì…€ 2 (ìˆ˜ì •ë¨): ë°ì´í„° ë¡œë“œ ë° ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
# ##############################################################################

# ë°ì´í„°ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
# Colabì— 'gesture_data_v3' í´ë”ë¥¼ ë§Œë“¤ê³  ê·¸ ì•ˆì— CSVë¥¼ ì—…ë¡œë“œí–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

# Google Drive ë§ˆìš´íŠ¸ (Colabì—ì„œ ì‹¤í–‰ ì‹œ)
from google.colab import drive
drive.mount('/content/drive')

DATA_DIR = '/content/drive/MyDrive/gesture_data_v3/'
file_pattern = "hybrid_typing_data_v3_*.csv"
all_files = glob.glob(os.path.join(DATA_DIR, file_pattern))

if not all_files:
    print(f"'{DATA_DIR}' í´ë”ì— '{file_pattern}' íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    print("Colab ì™¼í¸ì˜ íŒŒì¼ íƒìƒ‰ê¸°ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    # 1. ëª¨ë“  CSV íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    df_list = [pd.read_csv(f) for f in all_files]
    combined_df = pd.concat(df_list, ignore_index=True)
    print(f"ì´ {len(all_files)}ê°œì˜ íŒŒì¼ì—ì„œ {len(combined_df)}ê°œì˜ í”„ë ˆì„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    print(f"ì´ˆê¸° ë ˆì´ë¸” ë¶„í¬:\n{combined_df['label'].value_counts()}\n")

    # 2. í”¼ì²˜(X)ì™€ ë ˆì´ë¸”(y) ë¶„ë¦¬
    X_raw = combined_df.drop('label', axis=1).values

    # [ìˆ˜ì • ì‚¬í•­] ë ˆì´ë¸” ë°ì´í„°ë¥¼ ì •ìˆ˜(int) íƒ€ì…ìœ¼ë¡œ ëª…ì‹œì ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    y_raw = combined_df['label'].values.astype(int)

    # 3. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± í•¨ìˆ˜
    def create_sequences(X, y, sequence_length):
        """ì—°ì†ëœ í”„ë ˆì„ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        X_seq, y_seq = [], []
        # ì „ì²´ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©° ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë°ì´í„°ë¥¼ ë¬¶ìŠµë‹ˆë‹¤.
        for i in range(len(X) - sequence_length + 1):
            X_seq.append(X[i:i + sequence_length])
            # ë ˆì´ë¸”ì€ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ í”„ë ˆì„ì˜ ë ˆì´ë¸”ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            y_seq.append(y[i + sequence_length - 1])
        return np.array(X_seq), np.array(y_seq)

    # 4. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì‹¤í–‰
    X_sequences, y_sequences = create_sequences(X_raw, y_raw, config.SEQUENCE_LENGTH)

    # 5. TCN ëª¨ë¸ì˜ ì…ë ¥ ì±„ë„ ìˆ˜ë¥¼ ë°ì´í„°ì— ë§ê²Œ ë™ì ìœ¼ë¡œ ì„¤ì •
    config.TCN_INPUT_CHANNELS = X_sequences.shape[2]

    print("ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    print(f"X í˜•íƒœ: {X_sequences.shape}")
    print(f"y í˜•íƒœ: {y_sequences.shape}")
    print(f"ë³€í™˜ í›„ ë ˆì´ë¸” ë¶„í¬: {Counter(y_sequences)}")
    print(f"TCN ì…ë ¥ ì±„ë„ ìˆ˜ ì„¤ì •: {config.TCN_INPUT_CHANNELS}")

# ##############################################################################
# @title ì…€ 3 (ìˆ˜ì •ë¨): ê³µê°„ì  ë°ì´í„° ì¦ê°• ë° Custom Dataset ì •ì˜
# ##############################################################################

# --- ê³µê°„ì  ë°ì´í„° ì¦ê°• í•¨ìˆ˜ë“¤ ---
# ë³´ê³ ì„œ í‘œ 3ì˜ ê°€ì´ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ì‹¤ì œ 3D ëœë“œë§ˆí¬ì˜ íŠ¹ì„±ì— ë§ê²Œ ì„¸ë¶€ ë¡œì§ì„ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

def augment_rotate(sequence: np.ndarray, max_angle_deg: float = 10.0) -> np.ndarray:
    """
    3D ê³µê°„ì—ì„œ ì‹œí€€ìŠ¤ë¥¼ ë¬´ì‘ìœ„ë¡œ íšŒì „ì‹œí‚µë‹ˆë‹¤.
    [ìˆ˜ì • ì‚¬í•­] np.einsum ëŒ€ì‹  np.dotì„ ì‚¬ìš©í•˜ê³ , ë°ì´í„° ì°¨ì›ì„ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # (Seq_Len, 63) í˜•íƒœì˜ ì›ë³¸ ì‹œí€€ìŠ¤
    original_shape = sequence.shape
    if original_shape[1] % 3 != 0:
        # í”¼ì²˜ ìˆ˜ê°€ 3ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•Šìœ¼ë©´ 3D ì¢Œí‘œê°€ ì•„ë‹ˆë¯€ë¡œ íšŒì „í•˜ì§€ ì•ŠìŒ
        return sequence

    # (Seq_Len, 63) -> (Seq_Len, 21, 3)ìœ¼ë¡œ ë³€í™˜ (3D ì¢Œí‘œ 21ê°œë¼ê³  ê°€ì •)
    num_points = original_shape[1] // 3
    sequence_reshaped = sequence.reshape(original_shape[0], num_points, 3)

    angle_rad = np.random.uniform(-max_angle_deg, max_angle_deg) * (np.pi / 180.0)
    cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)

    # Zì¶• íšŒì „ í–‰ë ¬ (3, 3)
    # í•„ìš”ì‹œ X, Yì¶• íšŒì „ í–‰ë ¬ì„ ì¶”ê°€ë¡œ ê³±í•˜ì—¬ ë³µí•©ì ì¸ íšŒì „ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    rotation_matrix = np.array([
        [cos_a, -sin_a, 0],
        [sin_a,  cos_a, 0],
        [0,      0,     1]
    ])

    # np.dotì„ ì‚¬ìš©í•˜ì—¬ íšŒì „ ì ìš©
    # (Seq_Len, 21, 3)ê³¼ (3, 3) í–‰ë ¬ì„ ê³±í•©ë‹ˆë‹¤.
    rotated_sequence_reshaped = np.dot(sequence_reshaped, rotation_matrix)

    # ë‹¤ì‹œ ì›ë˜ í˜•íƒœ (Seq_Len, 63)ìœ¼ë¡œ ë³€í™˜
    rotated_sequence = rotated_sequence_reshaped.reshape(original_shape)

    return rotated_sequence

def augment_scale(sequence: np.ndarray, min_scale: float = 0.9, max_scale: float = 1.1) -> np.ndarray:
    """ì‹œí€€ìŠ¤ì˜ í¬ê¸°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì ˆí•©ë‹ˆë‹¤."""
    scale = np.random.uniform(min_scale, max_scale)
    return sequence * scale

def augment_jitter(sequence: np.ndarray, std: float = 0.01) -> np.ndarray:
    """ì‹œí€€ìŠ¤ì˜ ê° ì¢Œí‘œì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    noise = np.random.normal(loc=0.0, scale=std, size=sequence.shape)
    return sequence + noise

# --- ì˜¨ë¼ì¸ ì¦ê°•ì„ ì ìš©í•˜ëŠ” Custom Dataset í´ë˜ìŠ¤ ---
class GestureDataset(Dataset):
    def __init__(self, X, y, sequence_length, apply_augmentations=False):
        self.X = X
        self.y = y
        self.sequence_length = sequence_length
        self.apply_augmentations = apply_augmentations

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        # ì›ë³¸ ë°ì´í„°ëŠ” numpy í˜•íƒœì´ë¯€ë¡œ, í…ì„œë¡œ ë³€í™˜
        x_item = self.X[idx].copy() # ì›ë³¸ ìˆ˜ì •ì„ ë§‰ê¸° ìœ„í•´ ë³µì‚¬
        y_item = self.y[idx]

        # í›ˆë ¨(train) ëª¨ë“œì¼ ë•Œë§Œ ë°ì´í„° ì¦ê°•ì„ ì ìš©
        if self.apply_augmentations:
            # ë³´ê³ ì„œ í‘œ 3ì˜ ì¦ê°• ê¸°ë²•ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©
            x_item = augment_rotate(x_item)
            x_item = augment_scale(x_item)
            x_item = augment_jitter(x_item)

        # ìµœì¢…ì ìœ¼ë¡œ PyTorch í…ì„œë¡œ ë³€í™˜
        x_tensor = torch.FloatTensor(x_item)
        y_tensor = torch.LongTensor([y_item]).squeeze() # ìŠ¤ì¹¼ë¼ í…ì„œë¡œ ë³€í™˜

        return x_tensor, y_tensor

print("ê³µê°„ì  ë°ì´í„° ì¦ê°• í•¨ìˆ˜ ë° Custom Dataset í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆ˜ì • ì™„ë£Œ)")

# ##############################################################################
# @title ì…€ 4 (ìˆ˜ì •ë¨): TCN ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
# ##############################################################################

class TemporalBlock(nn.Module):
    """
    TCNì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì¸ Temporal Blockì…ë‹ˆë‹¤.
    [ìˆ˜ì • ì‚¬í•­] ì»¨ë³¼ë£¨ì…˜ ì´í›„ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•´ ìŠ¬ë¼ì´ì‹± ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()
        # nn.utils.weight_normì„ conv ë ˆì´ì–´ì— ì§ì ‘ ì ìš©í•©ë‹ˆë‹¤.
        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,
                                           stride=stride, padding=padding, dilation=dilation))
        self.padding1 = padding # ìŠ¬ë¼ì´ì‹±ì— ì‚¬ìš©í•  íŒ¨ë”© ê°’ì„ ì €ì¥
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,
                                           stride=stride, padding=padding, dilation=dilation))
        self.padding2 = padding # ìŠ¬ë¼ì´ì‹±ì— ì‚¬ìš©í•  íŒ¨ë”© ê°’ì„ ì €ì¥
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        # ë‹¤ìš´ìƒ˜í”Œë§ ë ˆì´ì–´ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ì˜ ì±„ë„ ìˆ˜ê°€ ë‹¤ë¥¼ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()

    def forward(self, x):
        # x: (Batch, Channels, Seq_Len)

        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        out = self.conv1(x)
        # ëŠ˜ì–´ë‚œ íŒ¨ë”©ë§Œí¼ ëë¶€ë¶„ì„ ì˜ë¼ë‚´ì–´ ê¸¸ì´ë¥¼ ë§ì¶¤ (Causal Convolution)
        out = out[:, :, :-self.padding1]
        out = self.relu1(out)
        out = self.dropout1(out)

        # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        out = self.conv2(out)
        out = out[:, :, :-self.padding2]
        out = self.relu2(out)
        out = self.dropout2(out)

        # ì”ì°¨ ì—°ê²° (Residual Connection)
        res = x if self.downsample is None else self.downsample(x)

        # ì´ì œ outê³¼ resì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë™ì¼í•˜ë¯€ë¡œ ë§ì…ˆì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        return self.relu(out + res)


class TCN(nn.Module):
    """
    Temporal Convolutional Network ëª¨ë¸ì…ë‹ˆë‹¤.
    ë³´ê³ ì„œ í‘œ 4ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì„±ë©ë‹ˆë‹¤.
    """
    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):
        super(TCN, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            # ì¸ê³¼ì  ì»¨ë³¼ë£¨ì…˜ì„ ìœ„í•œ íŒ¨ë”© ê³„ì‚°
            padding = (kernel_size - 1) * dilation_size
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,
                                     padding=padding, dropout=dropout)]

        self.tcn = nn.Sequential(*layers)
        self.linear = nn.Linear(num_channels[-1], output_size)

    def forward(self, x):
        # ì…ë ¥: (Batch, Seq_Len, Features)
        # TCNì€ (Batch, Features, Seq_Len) í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ì°¨ì› ë³€ê²½
        x = x.permute(0, 2, 1)

        out = self.tcn(x)

        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜
        out = self.linear(out[:, :, -1])
        return out

print("TCN ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆ˜ì • ì™„ë£Œ)")

# ##############################################################################
# @title ì…€ 5: í›ˆë ¨ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜
# ##############################################################################
from tqdm.auto import tqdm # ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

def train_one_epoch(model, data_loader, criterion, optimizer, device):
    """í•œ ì—í¬í¬ ë™ì•ˆ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” í•¨ìˆ˜"""
    model.train()
    total_loss = 0

    for inputs, labels in tqdm(data_loader, desc="Training", leave=False):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

def evaluate(model, data_loader, criterion, device):
    """ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(data_loader, desc="Evaluating", leave=False):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(data_loader)

    # ë³´ê³ ì„œì—ì„œ ê¶Œì¥í•˜ëŠ” F1-Scoreë¥¼ í¬í•¨í•œ í‰ê°€ì§€í‘œ ê³„ì‚° [cite: 91]
    metrics = {
        'loss': avg_loss,
        'f1_score': f1_score(all_labels, all_preds, average='binary'),
        'accuracy': accuracy_score(all_labels, all_preds),
        'precision': precision_score(all_labels, all_preds, average='binary'),
        'recall': recall_score(all_labels, all_preds, average='binary')
    }

    return metrics

print("í›ˆë ¨ ë° í‰ê°€ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 6 (ìˆ˜ì •ë¨): ğŸš€ 1ë‹¨ê³„ - ê¸°ì¤€ ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ##############################################################################
print("--- 1ë‹¨ê³„: ê¸°ì¤€ ëª¨ë¸(Baseline Model) í›ˆë ¨ ì‹œì‘ ---")

# 1. ë°ì´í„°ë¥¼ í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬ (80:20)
X_train, X_val, y_train, y_val = train_test_split(
    X_sequences, y_sequences,
    test_size=0.2,
    random_state=SEED,
    stratify=y_sequences
)

# 2. í›ˆë ¨ìš©/ê²€ì¦ìš© Dataset ë° DataLoader ìƒì„±
train_dataset = GestureDataset(X_train, y_train, config.SEQUENCE_LENGTH, apply_augmentations=True)
val_dataset = GestureDataset(X_val, y_val, config.SEQUENCE_LENGTH, apply_augmentations=False)
train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)
print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")

# 3. ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜
baseline_model = TCN(
    input_size=config.TCN_INPUT_CHANNELS,
    output_size=2,
    num_channels=config.TCN_NUM_CHANNELS,
    kernel_size=config.TCN_KERNEL_SIZE,
    dropout=config.DROPOUT
).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=config.LEARNING_RATE)

print("\nëª¨ë¸ ì•„í‚¤í…ì²˜:")
print(baseline_model)
print(f"\nì´ {config.BASELINE_EPOCHS} ì—í¬í¬ ë™ì•ˆ ê¸°ì¤€ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# 4. í›ˆë ¨ ë£¨í”„ ì‹¤í–‰
# [ìˆ˜ì • ì‚¬í•­] config.BASELINE_EPOCHS ì‚¬ìš©
for epoch in range(config.BASELINE_EPOCHS):
    train_loss = train_one_epoch(baseline_model, train_loader, criterion, optimizer, device)
    val_metrics = evaluate(baseline_model, val_loader, criterion, device)
    print(
        f"Epoch {epoch+1}/{config.BASELINE_EPOCHS} | "
        f"Train Loss: {train_loss:.4f} | "
        f"Val Loss: {val_metrics['loss']:.4f} | "
        f"Val F1: {val_metrics['f1_score']:.4f}"
    )

# 5. í›ˆë ¨ëœ ê¸°ì¤€ ëª¨ë¸ ì €ì¥
BASELINE_MODEL_PATH = 'baseline_model_tcn.pt'
torch.save(baseline_model.state_dict(), BASELINE_MODEL_PATH)

print("-" * 50)
print(f"1ë‹¨ê³„ ì™„ë£Œ: ê¸°ì¤€ ëª¨ë¸ì´ '{BASELINE_MODEL_PATH}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 7: T-SMOTE ì•Œê³ ë¦¬ì¦˜ í•¨ìˆ˜ ì •ì˜
# ##############################################################################

def t_smote(X, y, model, beta_param_a=2.0, beta_param_b=2.0):
    """
    ë³´ê³ ì„œ ë° ì›ë³¸ ë…¼ë¬¸ì— ê¸°ë°˜í•œ T-SMOTE ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.
    XëŠ” (ìƒ˜í”Œ ìˆ˜, í”¼ì²˜ ìˆ˜) í˜•íƒœì˜ 2D ë°°ì—´ì„ ê°€ì •í•©ë‹ˆë‹¤.
    ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ê²½ìš°, (ìƒ˜í”Œ ìˆ˜, ì‹œí€€ìŠ¤ ê¸¸ì´ * í”¼ì²˜ ìˆ˜)ë¡œ ë³€í™˜í•˜ì—¬ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

    Args:
        X (np.array): í”¼ì²˜ ë°ì´í„° (ìƒ˜í”Œ ìˆ˜, í”¼ì²˜ ìˆ˜)
        y (np.array): ë ˆì´ë¸” ë°ì´í„° (ìƒ˜í”Œ ìˆ˜,)
        model: ì‚¬ì „ í›ˆë ¨ëœ ë¶„ë¥˜ê¸°. `predict_proba` ë©”ì„œë“œê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        beta_param_a (float): ë² íƒ€ ë¶„í¬ì˜ ì•ŒíŒŒ íŒŒë¼ë¯¸í„°
        beta_param_b (float): ë² íƒ€ ë¶„í¬ì˜ ë² íƒ€ íŒŒë¼ë¯¸í„°

    Returns:
        X_resampled (np.array): T-SMOTEë¡œ ì¦ê°•ëœ í”¼ì²˜ ë°ì´í„°
        y_resampled (np.array): T-SMOTEë¡œ ì¦ê°•ëœ ë ˆì´ë¸” ë°ì´í„°
    """
    stats = Counter(y)
    minority_class = min(stats, key=stats.get)
    majority_class = max(stats, key=stats.get)

    n_samples_to_generate = stats[majority_class] - stats[minority_class]

    if n_samples_to_generate <= 0:
        print("ë°ì´í„°ê°€ ì´ë¯¸ ê· í˜•ì„ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤.")
        return X, y

    print(f"T-SMOTE ì‹œì‘: ì†Œìˆ˜ í´ë˜ìŠ¤({minority_class}) ìƒ˜í”Œ {n_samples_to_generate}ê°œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    X_minority = X[y == minority_class]

    # 1ë‹¨ê³„: ê²½ê³„ ê·¼ì ‘ í›„ë³´ ìƒì„± (Boundary Proximity Candidate Generation)
    print("1ë‹¨ê³„: ê²½ê³„ ê·¼ì ‘ í›„ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    # ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ì‚¬ìš©í•˜ì—¬ ê²½ê³„ì— ê°€ê¹Œìš´ ìƒ˜í”Œ(í›„ë³´)ì„ ì‹ë³„
    pred_scores = model.predict_proba(X_minority)[:, minority_class]
    # ì ìˆ˜ê°€ 0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ ë†’ì•„ì§
    candidate_weights = 1 - np.abs(pred_scores - 0.5)

    # ì‹œê°„ì  ì¸ì ‘ì„±ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ê° ì†Œìˆ˜ì  ìƒ˜í”Œì˜ ìµœê·¼ì ‘ ì´ì›ƒì„ ì°¾ìŒ
    nn = NearestNeighbors(n_neighbors=2, n_jobs=-1)
    nn.fit(X_minority)

    synthetic_samples = []

    # 2ë‹¨ê³„ & 3ë‹¨ê³„: í•©ì„± ë° ê°€ì¤‘ ìƒ˜í”Œë§ (Synthesis & Weighted Sampling)
    print("2ë‹¨ê³„ ë° 3ë‹¨ê³„: ìƒ˜í”Œ í•©ì„± ë° ê°€ì¤‘ ìƒ˜í”Œë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”í•˜ì—¬ ìƒ˜í”Œë§ í™•ë¥ ë¡œ ì‚¬ìš©
    sampling_probabilities = candidate_weights / np.sum(candidate_weights)

    # ê°€ì¤‘ì¹˜ì— ë”°ë¼ n_samples_to_generateê°œì˜ ì›ë³¸ ìƒ˜í”Œì„ ë³µì›ì¶”ì¶œ
    selected_indices = np.random.choice(
        np.arange(len(X_minority)),
        size=n_samples_to_generate,
        p=sampling_probabilities,
        replace=True
    )

    for i in tqdm(selected_indices, desc="Synthesizing Samples"):
        base_sample = X_minority[i]
        _, neighbor_indices = nn.kneighbors([base_sample])
        neighbor_sample = X_minority[neighbor_indices[0, 1]]

        # ë² íƒ€ ë¶„í¬ì— ë”°ë¼ ê°€ì¤‘ ë‚´ì‚½í•˜ì—¬ ìƒˆë¡œìš´ ìƒ˜í”Œ í•©ì„±
        beta = np.random.beta(beta_param_a, beta_param_b)
        new_sample = base_sample + beta * (neighbor_sample - base_sample)
        synthetic_samples.append(new_sample)

    if not synthetic_samples:
        print("í•©ì„±ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return X, y

    X_resampled = np.vstack([X, np.array(synthetic_samples)])
    y_resampled = np.hstack([y, np.full(n_samples_to_generate, minority_class)])

    print("\nT-SMOTE ì™„ë£Œ!")
    print(f"ì´ì „ ë°ì´í„° ë¶„í¬: {stats}")
    print(f"ì´í›„ ë°ì´í„° ë¶„í¬: {Counter(y_resampled)}")

    return X_resampled, y_resampled

print("T-SMOTE ì•Œê³ ë¦¬ì¦˜ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 8: PyTorch ëª¨ë¸ ë˜í¼(Wrapper) ì •ì˜
# ##############################################################################

class PyTorchModelWrapper:
    """
    PyTorch ëª¨ë¸ì„ t_smote í•¨ìˆ˜ì™€ í˜¸í™˜ë˜ë„ë¡ ê°ì‹¸ëŠ” ë˜í¼ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self, model_path, model_class, config, device):
        self.config = config
        self.device = device

        # ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°ë¥¼ ì´ˆê¸°í™”
        self.model = model_class(
            input_size=config.TCN_INPUT_CHANNELS,
            output_size=2,
            num_channels=config.TCN_NUM_CHANNELS,
            kernel_size=config.TCN_KERNEL_SIZE,
            dropout=config.DROPOUT
        ).to(device)

        # ì €ì¥ëœ 1ë‹¨ê³„ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜´
        self.model.load_state_dict(torch.load(model_path, map_location=device))
        # ë°˜ë“œì‹œ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•´ì•¼ ë“œë¡­ì•„ì›ƒ ë“±ì´ ë¹„í™œì„±í™”ë¨
        self.model.eval()
        print(f"'{model_path}'ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")

    def predict_proba(self, X_numpy):
        """
        Numpy ë°°ì—´ì„ ì…ë ¥ë°›ì•„ ì˜ˆì¸¡ í™•ë¥ ì„ Numpy ë°°ì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        T-SMOTEëŠ” 2D ë°ì´í„°ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ, ì‹œí€€ìŠ¤ë¥¼ 2Dë¡œ í¼ì³ì„œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        """
        # (Samples, Seq_Len, Features) -> (Samples, Seq_Len * Features)
        if X_numpy.ndim == 3:
            X_numpy = X_numpy.reshape(X_numpy.shape[0], -1)

        # ê° ìƒ˜í”Œì„ ë‹¤ì‹œ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³µì›
        # (Samples, Seq_Len * Features) -> (Samples, Seq_Len, Features)
        X_reshaped = X_numpy.reshape(-1, self.config.SEQUENCE_LENGTH, self.config.TCN_INPUT_CHANNELS)
        X_tensor = torch.FloatTensor(X_reshaped).to(self.device)

        with torch.no_grad():
            logits = self.model(X_tensor)
            # ë¡œì§“(logit)ì„ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ í†µí•´ í™•ë¥ ë¡œ ë³€í™˜
            probabilities = torch.softmax(logits, dim=1).cpu().numpy()

        return probabilities

print("PyTorch ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 9: 2ë‹¨ê³„ - T-SMOTE ë°ì´í„°ì…‹ ìƒì„± ì‹¤í–‰
# ##############################################################################
print("--- 2ë‹¨ê³„: T-SMOTE ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ ---")

# 1. 1ë‹¨ê³„ì—ì„œ í›ˆë ¨ëœ ê¸°ì¤€ ëª¨ë¸ì„ ë˜í¼ í´ë˜ìŠ¤ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
trained_model_wrapper = PyTorchModelWrapper(
    model_path=BASELINE_MODEL_PATH,
    model_class=TCN,
    config=config,
    device=device
)

# 2. T-SMOTEì— ì…ë ¥í•˜ê¸° ìœ„í•´ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ 2Dë¡œ í¼ì¹˜ê¸°
# (Samples, Seq_Len, Features) -> (Samples, Seq_Len * Features)
X_sequences_flat = X_sequences.reshape(X_sequences.shape[0], -1)

# 3. T-SMOTE ì‹¤í–‰
# ì…ë ¥: í¼ì³ì§„ 2D ì›ë³¸ ë°ì´í„°, ì›ë³¸ ë ˆì´ë¸”, í›ˆë ¨ëœ ëª¨ë¸ ë˜í¼
X_resampled_flat, y_resampled = t_smote(
    X_sequences_flat,
    y_sequences,
    model=trained_model_wrapper
)

# 4. ìƒì„±ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³µì›
# (Samples, Seq_Len * Features) -> (Samples, Seq_Len, Features)
X_resampled = X_resampled_flat.reshape(
    -1,
    config.SEQUENCE_LENGTH,
    config.TCN_INPUT_CHANNELS
)

# 5. ìƒì„±ëœ ê· í˜• ë°ì´í„°ì…‹ì„ .npz íŒŒì¼ë¡œ ì €ì¥
BALANCED_DATASET_PATH = 'balanced_dataset_tcn.npz'
np.savez_compressed(
    BALANCED_DATASET_PATH,
    X=X_resampled,
    y=y_resampled
)

print("-" * 50)
print(f"X Resampled Shape: {X_resampled.shape}")
print(f"y Resampled Shape: {y_resampled.shape}")
print(f"2ë‹¨ê³„ ì™„ë£Œ: ê· í˜• ë°ì´í„°ì…‹ì´ '{BALANCED_DATASET_PATH}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 10: 3ë‹¨ê³„ - ìµœì¢… í›ˆë ¨ ì¤€ë¹„
# ##############################################################################
print("--- 3ë‹¨ê³„: ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤€ë¹„ ---")

# 1. 2ë‹¨ê³„ì—ì„œ ì €ì¥í•œ ê· í˜• ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
balanced_data = np.load(BALANCED_DATASET_PATH)
X_balanced = balanced_data['X']
y_balanced = balanced_data['y']

print(f"ë¶ˆëŸ¬ì˜¨ ê· í˜• ë°ì´í„°ì…‹ ë ˆì´ë¸” ë¶„í¬: {Counter(y_balanced)}")

# 2. ê· í˜• ë°ì´í„°ì…‹ì„ ìµœì¢… í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬ (80:20)
# ë°ì´í„°ê°€ ì´ë¯¸ ê· í˜•ì„ ì´ë£¨ë¯€ë¡œ stratify ì˜µì…˜ì€ í•„ìˆ˜ê°€ ì•„ë‹ˆì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(
    X_balanced, y_balanced,
    test_size=0.2,
    random_state=SEED,
    stratify=y_balanced
)

# 3. í›ˆë ¨ìš© Datasetì€ ì¦ê°•ì„ ì ìš©(True), ê²€ì¦ìš©ì€ ë¯¸ì ìš©(False)
final_train_dataset = GestureDataset(X_train_final, y_train_final, config.SEQUENCE_LENGTH, apply_augmentations=True)
final_val_dataset = GestureDataset(X_val_final, y_val_final, config.SEQUENCE_LENGTH, apply_augmentations=False)

final_train_loader = DataLoader(final_train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
final_val_loader = DataLoader(final_val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)

print(f"ìµœì¢… í›ˆë ¨ ë°ì´í„°: {len(final_train_dataset)}ê°œ, ìµœì¢… ê²€ì¦ ë°ì´í„°: {len(final_val_dataset)}ê°œ")

# ##############################################################################
# @title ì…€ 11 (ìˆ˜ì •ë¨): 3ë‹¨ê³„ - ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ##############################################################################
print("--- 3ë‹¨ê³„: ìµœì¢… ëª¨ë¸(Final Model) í›ˆë ¨ ì‹œì‘ ---")

# 1. ìµœì¢… ëª¨ë¸ì„ ìƒˆë¡­ê²Œ ì´ˆê¸°í™”
final_model = TCN(
    input_size=config.TCN_INPUT_CHANNELS,
    output_size=2,
    num_channels=config.TCN_NUM_CHANNELS,
    kernel_size=config.TCN_KERNEL_SIZE,
    dropout=config.DROPOUT
).to(device)

# 2. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(final_model.parameters(), lr=config.LEARNING_RATE)

print("\nìµœì¢… ëª¨ë¸ ì•„í‚¤í…ì²˜:")
print(final_model)
print(f"\nì´ {config.FINAL_EPOCHS} ì—í¬í¬ ë™ì•ˆ ìµœì¢… ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# 3. í›ˆë ¨ ë£¨í”„ ì‹¤í–‰
best_f1_score = 0.0
FINAL_MODEL_PATH = 'final_model_tcn.pt'

# [ìˆ˜ì • ì‚¬í•­] config.FINAL_EPOCHS ì‚¬ìš©
for epoch in range(config.FINAL_EPOCHS):
    train_loss = train_one_epoch(final_model, final_train_loader, criterion, optimizer, device)
    val_metrics = evaluate(final_model, final_val_loader, criterion, device)

    print(
        f"Epoch {epoch+1}/{config.FINAL_EPOCHS} | "
        f"Train Loss: {train_loss:.4f} | "
        f"Val Loss: {val_metrics['loss']:.4f} | "
        f"Val F1: {val_metrics['f1_score']:.4f} | "
        f"Val Acc: {val_metrics['accuracy']:.4f}"
    )

    if val_metrics['f1_score'] > best_f1_score:
        best_f1_score = val_metrics['f1_score']
        torch.save(final_model.state_dict(), FINAL_MODEL_PATH)
        print(f"  -> New best model saved with F1-Score: {best_f1_score:.4f}")

print("-" * 50)
print(f"3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… ëª¨ë¸ì´ '{FINAL_MODEL_PATH}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ìµœê³  ê²€ì¦ F1-Score: {best_f1_score:.4f}")

