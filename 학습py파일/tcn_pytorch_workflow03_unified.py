# -*- coding: utf-8 -*-
"""TCN_pytorch_workflow03_unified.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J3iDgYBSkk5uIon7D3vT1h69Zf2fvuBV
"""

# ##############################################################################
# @title ì…€ 1 (ìˆ˜ì •ë¨): ê¸°ë³¸ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ##############################################################################
import os
import glob
import pandas as pd
import numpy as np
from collections import Counter

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
print("-" * 50)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
class Config:
    # --- ë°ì´í„° ê´€ë ¨ ---
    SEQUENCE_LENGTH = 20

    # --- í›ˆë ¨ ê´€ë ¨ ---
    OPTIMIZER = 'AdamW'
    LEARNING_RATE = 1e-4
    BATCH_SIZE = 64
    BASELINE_EPOCHS = 20
    FINAL_EPOCHS = 50

    # --- ëª¨ë¸ ì•„í‚¤í…ì²˜ ê´€ë ¨ (TCN) ---
    TCN_INPUT_CHANNELS = -1
    TCN_NUM_CHANNELS = [128] * 2
    TCN_KERNEL_SIZE = 3
    DROPOUT = 0.2

    # [ìˆ˜ì •] T-SMOTE ìƒ˜í”Œë§ ì „ëµ íŒŒë¼ë¯¸í„° ì¶”ê°€
    # ë¦¬ìƒ˜í”Œë§ í›„ (ì†Œìˆ˜ í´ë˜ìŠ¤ ìˆ˜ / ë‹¤ìˆ˜ í´ë˜ìŠ¤ ìˆ˜) ëª©í‘œ ë¹„ìœ¨.
    # 1.0     -> 1:1 ë¹„ìœ¨
    # 3 / 7   -> 7:3 ë¹„ìœ¨
    # 4 / 6   -> 6:4 ë¹„ìœ¨
    T_SMOTE_SAMPLING_STRATEGY = 3 / 7 # ì˜ˆ: 7:3 ë¹„ìœ¨ë¡œ ì„¤ì •

# ì„¤ì •ê°’ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
config = Config()

# ##############################################################################
# @title ì…€ 2 (í†µí•© ëª¨ë¸ìš©): ë°ì´í„° ë¡œë”© (ì–‘ì† í†µí•©)
# ##############################################################################

# ë°ì´í„°ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
# Colabì— í´ë”ë¥¼ ë§Œë“¤ê³  ê·¸ ì•ˆì— CSVë¥¼ ì—…ë¡œë“œí–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

# Google Drive ë§ˆìš´íŠ¸ (Colabì—ì„œ ì‹¤í–‰ ì‹œ)
from google.colab import drive
drive.mount('/content/drive')

# [ë³€ê²½ì ] _left, _right í´ë” ëª¨ë‘ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ë„ë¡ ê²½ë¡œ íŒ¨í„´ ìˆ˜ì •
DATA_DIR_PATTERN = '/content/drive/MyDrive/gesture_data_v3_*'
# í´ë” ë‚´ì˜ íŒŒì¼ ì´ë¦„ì€ ë™ì¼í•œ ì ‘ë‘ì‚¬ë¥¼ ê°€ì§
FILE_PATTERN = 'hybrid_typing_data_v3_*.csv'

# globì„ ë‘ ë²ˆ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í•˜ìœ„ í´ë”ì˜ ëª¨ë“  csv íŒŒì¼ì„ ì°¾ìŒ
all_folders = glob.glob(DATA_DIR_PATTERN)
all_files = []
for folder in all_folders:
    files_in_folder = glob.glob(os.path.join(folder, FILE_PATTERN))
    all_files.extend(files_in_folder)

if not all_files:
    print(f"'{DATA_DIR_PATTERN}' íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” í´ë”ë‚˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    print("ë°ì´í„° ìˆ˜ì§‘ í´ë”(ì˜ˆ: hybrid_typing_data_v3_left)ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    # 1. ëª¨ë“  CSV íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    df_list = [pd.read_csv(f) for f in all_files]
    combined_df = pd.concat(df_list, ignore_index=True)
    print(f"ì´ {len(all_folders)}ê°œ í´ë”, {len(all_files)}ê°œì˜ íŒŒì¼ì—ì„œ {len(combined_df)}ê°œì˜ í”„ë ˆì„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    print(f"ì´ˆê¸° ë ˆì´ë¸” ë¶„í¬:\n{combined_df['label'].value_counts()}\n")

    # 2. í”¼ì²˜(X)ì™€ ë ˆì´ë¸”(y) ë¶„ë¦¬ ë° íƒ€ì… ë³€í™˜
    X_raw = combined_df.drop('label', axis=1).values
    y_raw = combined_df['label'].values.astype(int)

    # 3. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
    def create_sequences(X, y, sequence_length):
        X_seq, y_seq = [], []
        for i in range(len(X) - sequence_length + 1):
            X_seq.append(X[i:i + sequence_length])
            y_seq.append(y[i + sequence_length - 1])
        return np.array(X_seq), np.array(y_seq)

    # 4. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì‹¤í–‰
    X_sequences, y_sequences = create_sequences(X_raw, y_raw, config.SEQUENCE_LENGTH)

    # 5. ëª¨ë¸ì˜ ì…ë ¥ ì±„ë„ ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •
    config.TCN_INPUT_CHANNELS = X_sequences.shape[2]

    print("ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    print(f"X í˜•íƒœ: {X_sequences.shape}")
    print(f"y í˜•íƒœ: {y_sequences.shape}")
    print(f"ë³€í™˜ í›„ ë ˆì´ë¸” ë¶„í¬: {Counter(y_sequences)}")

# ##############################################################################
# @title ì…€ 3 (í†µí•© ëª¨ë¸ìš©): ë°ì´í„° ì¦ê°• (ì¢Œìš° ë°˜ì „ ì¶”ê°€)
# ##############################################################################

def augment_horizontal_flip(sequence: np.ndarray) -> np.ndarray:
    """
    ëœë“œë§ˆí¬ ì‹œí€€ìŠ¤ë¥¼ Y-Z í‰ë©´(x=0)ì„ ê¸°ì¤€ìœ¼ë¡œ ì¢Œìš° ë°˜ì „ì‹œí‚µë‹ˆë‹¤.
    x ì¢Œí‘œì— -1ì„ ê³±í•˜ì—¬ ê°„ë‹¨íˆ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    flipped_sequence = sequence.copy()
    # x ì¢Œí‘œëŠ” 3ê°œë§ˆë‹¤ í•˜ë‚˜ì”© ìœ„ì¹˜ (x0, y0, z0, x1, y1, z1, ...)
    # [:, ::3]ëŠ” ëª¨ë“  í”„ë ˆì„ì— ëŒ€í•´, 0ë²ˆ featureë¶€í„° 3ì¹¸ì”© ê±´ë„ˆë›°ë©° ì„ íƒ
    flipped_sequence[:, ::3] = -1 * flipped_sequence[:, ::3]
    return flipped_sequence

def augment_rotate(sequence: np.ndarray, max_angle_deg: float = 10.0) -> np.ndarray:
    # (ì´ì „ê³¼ ë™ì¼í•œ ì½”ë“œ)
    original_shape = sequence.shape
    if original_shape[1] % 3 != 0: return sequence
    num_points = original_shape[1] // 3
    sequence_reshaped = sequence.reshape(original_shape[0], num_points, 3)
    angle_rad = np.random.uniform(-max_angle_deg, max_angle_deg) * (np.pi / 180.0)
    cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)
    rotation_matrix = np.array([[cos_a, -sin_a, 0], [sin_a, cos_a, 0], [0, 0, 1]])
    rotated_sequence_reshaped = np.dot(sequence_reshaped, rotation_matrix)
    return rotated_sequence_reshaped.reshape(original_shape)

def augment_scale(sequence: np.ndarray, min_scale: float = 0.9, max_scale: float = 1.1) -> np.ndarray:
    # (ì´ì „ê³¼ ë™ì¼í•œ ì½”ë“œ)
    scale = np.random.uniform(min_scale, max_scale)
    return sequence * scale

def augment_jitter(sequence: np.ndarray, std: float = 0.01) -> np.ndarray:
    # (ì´ì „ê³¼ ë™ì¼í•œ ì½”ë“œ)
    noise = np.random.normal(loc=0.0, scale=std, size=sequence.shape)
    return sequence + noise

# --- ì˜¨ë¼ì¸ ì¦ê°•ì„ ì ìš©í•˜ëŠ” Custom Dataset í´ë˜ìŠ¤ ---
class GestureDataset(Dataset):
    def __init__(self, X, y, sequence_length, apply_augmentations=False):
        self.X = X
        self.y = y
        self.sequence_length = sequence_length
        self.apply_augmentations = apply_augmentations

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x_item = self.X[idx].copy()
        y_item = self.y[idx]

        if self.apply_augmentations:
            # [í•µì‹¬ ë³€ê²½ì ] 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ ì¦ê°•ì„ ë¨¼ì € ì ìš©
            if np.random.random() < 0.5:
                x_item = augment_horizontal_flip(x_item)

            # ê¸°ì¡´ì˜ ê³µê°„ì  ì¦ê°•ë“¤ì„ì´ì–´ì„œ ì ìš©
            x_item = augment_rotate(x_item)
            x_item = augment_scale(x_item)
            x_item = augment_jitter(x_item)

        x_tensor = torch.FloatTensor(x_item)
        y_tensor = torch.LongTensor([y_item]).squeeze()

        return x_tensor, y_tensor

print("ë°ì´í„° ì¦ê°• í•¨ìˆ˜ ë° Custom Dataset í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ì¢Œìš° ë°˜ì „ ì¶”ê°€ ì™„ë£Œ)")

# ##############################################################################
# @title ì…€ 4 (ìˆ˜ì •ë¨): TCN ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
# ##############################################################################

class TemporalBlock(nn.Module):
    """
    TCNì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì¸ Temporal Blockì…ë‹ˆë‹¤.
    [ìˆ˜ì • ì‚¬í•­] ì»¨ë³¼ë£¨ì…˜ ì´í›„ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•´ ìŠ¬ë¼ì´ì‹± ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()
        # nn.utils.weight_normì„ conv ë ˆì´ì–´ì— ì§ì ‘ ì ìš©í•©ë‹ˆë‹¤.
        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,
                                           stride=stride, padding=padding, dilation=dilation))
        self.padding1 = padding # ìŠ¬ë¼ì´ì‹±ì— ì‚¬ìš©í•  íŒ¨ë”© ê°’ì„ ì €ì¥
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,
                                           stride=stride, padding=padding, dilation=dilation))
        self.padding2 = padding # ìŠ¬ë¼ì´ì‹±ì— ì‚¬ìš©í•  íŒ¨ë”© ê°’ì„ ì €ì¥
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        # ë‹¤ìš´ìƒ˜í”Œë§ ë ˆì´ì–´ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ì˜ ì±„ë„ ìˆ˜ê°€ ë‹¤ë¥¼ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()

    def forward(self, x):
        # x: (Batch, Channels, Seq_Len)

        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        out = self.conv1(x)
        # ëŠ˜ì–´ë‚œ íŒ¨ë”©ë§Œí¼ ëë¶€ë¶„ì„ ì˜ë¼ë‚´ì–´ ê¸¸ì´ë¥¼ ë§ì¶¤ (Causal Convolution)
        out = out[:, :, :-self.padding1]
        out = self.relu1(out)
        out = self.dropout1(out)

        # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        out = self.conv2(out)
        out = out[:, :, :-self.padding2]
        out = self.relu2(out)
        out = self.dropout2(out)

        # ì”ì°¨ ì—°ê²° (Residual Connection)
        res = x if self.downsample is None else self.downsample(x)

        # ì´ì œ outê³¼ resì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë™ì¼í•˜ë¯€ë¡œ ë§ì…ˆì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        return self.relu(out + res)


class TCN(nn.Module):
    """
    Temporal Convolutional Network ëª¨ë¸ì…ë‹ˆë‹¤.
    ë³´ê³ ì„œ í‘œ 4ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì„±ë©ë‹ˆë‹¤.
    """
    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):
        super(TCN, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            # ì¸ê³¼ì  ì»¨ë³¼ë£¨ì…˜ì„ ìœ„í•œ íŒ¨ë”© ê³„ì‚°
            padding = (kernel_size - 1) * dilation_size
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,
                                     padding=padding, dropout=dropout)]

        self.tcn = nn.Sequential(*layers)
        self.linear = nn.Linear(num_channels[-1], output_size)

    def forward(self, x):
        # ì…ë ¥: (Batch, Seq_Len, Features)
        # TCNì€ (Batch, Features, Seq_Len) í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ì°¨ì› ë³€ê²½
        x = x.permute(0, 2, 1)

        out = self.tcn(x)

        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜
        out = self.linear(out[:, :, -1])
        return out

print("TCN ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆ˜ì • ì™„ë£Œ)")

# ##############################################################################
# @title ì…€ 5: í›ˆë ¨ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜
# ##############################################################################
from tqdm.auto import tqdm # ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

def train_one_epoch(model, data_loader, criterion, optimizer, device):
    """í•œ ì—í¬í¬ ë™ì•ˆ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” í•¨ìˆ˜"""
    model.train()
    total_loss = 0

    for inputs, labels in tqdm(data_loader, desc="Training", leave=False):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

def evaluate(model, data_loader, criterion, device):
    """ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(data_loader, desc="Evaluating", leave=False):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(data_loader)

    # ë³´ê³ ì„œì—ì„œ ê¶Œì¥í•˜ëŠ” F1-Scoreë¥¼ í¬í•¨í•œ í‰ê°€ì§€í‘œ ê³„ì‚° [cite: 91]
    metrics = {
        'loss': avg_loss,
        'f1_score': f1_score(all_labels, all_preds, average='binary'),
        'accuracy': accuracy_score(all_labels, all_preds),
        'precision': precision_score(all_labels, all_preds, average='binary'),
        'recall': recall_score(all_labels, all_preds, average='binary')
    }

    return metrics

print("í›ˆë ¨ ë° í‰ê°€ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 6 (í†µí•© ëª¨ë¸ìš©): ğŸš€ 1ë‹¨ê³„ - ê¸°ì¤€ ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ##############################################################################
print("--- 1ë‹¨ê³„: í†µí•© ê¸°ì¤€ ëª¨ë¸(Baseline Model) í›ˆë ¨ ì‹œì‘ ---")

# 1. í†µí•© ë°ì´í„°ë¥¼ í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬
X_train, X_val, y_train, y_val = train_test_split(
    X_sequences, y_sequences,
    test_size=0.2,
    random_state=SEED,
    stratify=y_sequences
)

# 2. í›ˆë ¨ìš© Datasetì€ ì¦ê°•(ì¢Œìš° ë°˜ì „ í¬í•¨)ì„ ì ìš©, ê²€ì¦ìš©ì€ ë¯¸ì ìš©
train_dataset = GestureDataset(X_train, y_train, config.SEQUENCE_LENGTH, apply_augmentations=True)
val_dataset = GestureDataset(X_val, y_val, config.SEQUENCE_LENGTH, apply_augmentations=False)

train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)
print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")

# 3. ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜
baseline_model = TCN(
    input_size=config.TCN_INPUT_CHANNELS,
    output_size=2,
    num_channels=config.TCN_NUM_CHANNELS,
    kernel_size=config.TCN_KERNEL_SIZE,
    dropout=config.DROPOUT
).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=config.LEARNING_RATE)
print(f"\nì´ {config.BASELINE_EPOCHS} ì—í¬í¬ ë™ì•ˆ ê¸°ì¤€ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# 4. í›ˆë ¨ ë£¨í”„ ì‹¤í–‰
for epoch in range(config.BASELINE_EPOCHS):
    train_loss = train_one_epoch(baseline_model, train_loader, criterion, optimizer, device)
    val_metrics = evaluate(baseline_model, val_loader, criterion, device)
    print(
        f"Epoch {epoch+1}/{config.BASELINE_EPOCHS} | "
        f"Train Loss: {train_loss:.4f} | "
        f"Val F1: {val_metrics['f1_score']:.4f}"
    )

# 5. í›ˆë ¨ëœ ê¸°ì¤€ ëª¨ë¸ ì €ì¥ (íŒŒì¼ ì´ë¦„ ë³€ê²½)
BASELINE_MODEL_PATH = 'baseline_model_unified.pt'
torch.save(baseline_model.state_dict(), BASELINE_MODEL_PATH)

print("-" * 50)
print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: í†µí•© ê¸°ì¤€ ëª¨ë¸ì´ '{BASELINE_MODEL_PATH}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 7 (ìˆ˜ì •ë¨): T-SMOTE ì•Œê³ ë¦¬ì¦˜ í•¨ìˆ˜ ì •ì˜ (ì§„í–‰ë¥  í‘œì‹œ ì¶”ê°€)
# ##############################################################################

def t_smote(X, y, model, sampling_strategy=1.0, beta_param_a=2.0, beta_param_b=2.0):
    """
    ë¹„ìœ¨ ì¡°ì ˆ ë° ì§„í–‰ë¥  í‘œì‹œ ê¸°ëŠ¥ì´ í¬í•¨ëœ T-SMOTE ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.
    """
    stats = Counter(y)
    minority_class = min(stats, key=stats.get)
    majority_class = max(stats, key=stats.get)
    minority_count = stats[minority_class]
    majority_count = stats[majority_class]

    desired_minority_count = int(majority_count * sampling_strategy)
    n_samples_to_generate = desired_minority_count - minority_count

    if n_samples_to_generate <= 0:
        print(f"ì´ë¯¸ ëª©í‘œ ë¹„ìœ¨({sampling_strategy:.2f}) ì´ìƒìœ¼ë¡œ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. T-SMOTEë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return X, y

    print(f"T-SMOTE ì‹œì‘: ì†Œìˆ˜ í´ë˜ìŠ¤({minority_class}) ìƒ˜í”Œ {n_samples_to_generate}ê°œë¥¼ ìƒì„±í•˜ì—¬ ëª©í‘œ ë¹„ìœ¨ {sampling_strategy:.2f}ë¥¼ ë§ì¶¥ë‹ˆë‹¤.")

    X_minority = X[y == minority_class]
    pred_scores = model.predict_proba(X_minority)[:, minority_class]
    candidate_weights = 1 - np.abs(pred_scores - 0.5)

    nn = NearestNeighbors(n_neighbors=2, n_jobs=-1)
    nn.fit(X_minority)

    synthetic_samples = []

    sampling_probabilities = candidate_weights / np.sum(candidate_weights)
    selected_indices = np.random.choice(
        np.arange(len(X_minority)),
        size=n_samples_to_generate,
        p=sampling_probabilities,
        replace=True
    )

    # [ìˆ˜ì •] for ë£¨í”„ë¥¼ tqdmìœ¼ë¡œ ê°ì‹¸ì„œ ì§„í–‰ë¥ ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    for i in tqdm(selected_indices, desc="Synthesizing Samples"):
        base_sample = X_minority[i]
        _, neighbor_indices = nn.kneighbors([base_sample])
        neighbor_sample = X_minority[neighbor_indices[0, 1]]

        beta = np.random.beta(beta_param_a, beta_param_b)
        new_sample = base_sample + beta * (neighbor_sample - base_sample)
        synthetic_samples.append(new_sample)

    if not synthetic_samples:
        return X, y

    X_resampled = np.vstack([X, np.array(synthetic_samples)])
    y_resampled = np.hstack([y, np.full(n_samples_to_generate, minority_class)])

    print("\nT-SMOTE ì™„ë£Œ!")
    print(f"ì´ì „ ë°ì´í„° ë¶„í¬: {stats}")
    print(f"ì´í›„ ë°ì´í„° ë¶„í¬: {Counter(y_resampled)}")

    return X_resampled, y_resampled

print("T-SMOTE ì•Œê³ ë¦¬ì¦˜ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ì§„í–‰ë¥  í‘œì‹œ ì¶”ê°€ ì™„ë£Œ)")

# ##############################################################################
# @title ì…€ 8: PyTorch ëª¨ë¸ ë˜í¼(Wrapper) ì •ì˜
# ##############################################################################

class PyTorchModelWrapper:
    """
    PyTorch ëª¨ë¸ì„ t_smote í•¨ìˆ˜ì™€ í˜¸í™˜ë˜ë„ë¡ ê°ì‹¸ëŠ” ë˜í¼ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self, model_path, model_class, config, device):
        self.config = config
        self.device = device

        # ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°ë¥¼ ì´ˆê¸°í™”
        self.model = model_class(
            input_size=config.TCN_INPUT_CHANNELS,
            output_size=2,
            num_channels=config.TCN_NUM_CHANNELS,
            kernel_size=config.TCN_KERNEL_SIZE,
            dropout=config.DROPOUT
        ).to(device)

        # ì €ì¥ëœ 1ë‹¨ê³„ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜´
        self.model.load_state_dict(torch.load(model_path, map_location=device))
        # ë°˜ë“œì‹œ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•´ì•¼ ë“œë¡­ì•„ì›ƒ ë“±ì´ ë¹„í™œì„±í™”ë¨
        self.model.eval()
        print(f"'{model_path}'ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")

    def predict_proba(self, X_numpy):
        """
        Numpy ë°°ì—´ì„ ì…ë ¥ë°›ì•„ ì˜ˆì¸¡ í™•ë¥ ì„ Numpy ë°°ì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        T-SMOTEëŠ” 2D ë°ì´í„°ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ, ì‹œí€€ìŠ¤ë¥¼ 2Dë¡œ í¼ì³ì„œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        """
        # (Samples, Seq_Len, Features) -> (Samples, Seq_Len * Features)
        if X_numpy.ndim == 3:
            X_numpy = X_numpy.reshape(X_numpy.shape[0], -1)

        # ê° ìƒ˜í”Œì„ ë‹¤ì‹œ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³µì›
        # (Samples, Seq_Len * Features) -> (Samples, Seq_Len, Features)
        X_reshaped = X_numpy.reshape(-1, self.config.SEQUENCE_LENGTH, self.config.TCN_INPUT_CHANNELS)
        X_tensor = torch.FloatTensor(X_reshaped).to(self.device)

        with torch.no_grad():
            logits = self.model(X_tensor)
            # ë¡œì§“(logit)ì„ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ í†µí•´ í™•ë¥ ë¡œ ë³€í™˜
            probabilities = torch.softmax(logits, dim=1).cpu().numpy()

        return probabilities

print("PyTorch ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 9 (ìˆ˜ì •ë¨): ğŸš€ 2ë‹¨ê³„ - T-SMOTE ë°ì´í„°ì…‹ ìƒì„± ì‹¤í–‰
# ##############################################################################
print("--- 2ë‹¨ê³„: í†µí•© T-SMOTE ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ ---")

# 1. ê¸°ì¤€ ëª¨ë¸ì„ ë˜í¼ í´ë˜ìŠ¤ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
trained_model_wrapper = PyTorchModelWrapper(
    model_path=BASELINE_MODEL_PATH,
    model_class=TCN,
    config=config,
    device=device
)

# 2. ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ 2Dë¡œ í¼ì¹˜ê¸°
X_sequences_flat = X_sequences.reshape(X_sequences.shape[0], -1)

# 3. T-SMOTE ì‹¤í–‰
# [ìˆ˜ì •] sampling_strategy ê°’ì„ config ê°ì²´ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´
X_resampled_flat, y_resampled = t_smote(
    X_sequences_flat,
    y_sequences,
    model=trained_model_wrapper,
    sampling_strategy=config.T_SMOTE_SAMPLING_STRATEGY
)

# 4. ìƒì„±ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³µì›
X_resampled = X_resampled_flat.reshape(
    -1,
    config.SEQUENCE_LENGTH,
    config.TCN_INPUT_CHANNELS
)

# 5. ìƒì„±ëœ ê· í˜• ë°ì´í„°ì…‹ì„ .npz íŒŒì¼ë¡œ ì €ì¥
BALANCED_DATASET_PATH = 'balanced_dataset_unified.npz'
np.savez_compressed(
    BALANCED_DATASET_PATH,
    X=X_resampled,
    y=y_resampled
)

print("-" * 50)
print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: í†µí•© ê· í˜• ë°ì´í„°ì…‹ì´ '{BALANCED_DATASET_PATH}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ##############################################################################
# @title ì…€ 10 (í†µí•© ëª¨ë¸ìš©): 3ë‹¨ê³„ - ìµœì¢… í›ˆë ¨ ì¤€ë¹„
# ##############################################################################
print("--- 3ë‹¨ê³„: ìµœì¢… í†µí•© ëª¨ë¸(Final Unified Model) í›ˆë ¨ ì¤€ë¹„ ---")

# 1. 2ë‹¨ê³„ì—ì„œ ì €ì¥í•œ ê· í˜• ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
balanced_data = np.load(BALANCED_DATASET_PATH)
X_balanced = balanced_data['X']
y_balanced = balanced_data['y']
print(f"ë¶ˆëŸ¬ì˜¨ ê· í˜• ë°ì´í„°ì…‹ ë ˆì´ë¸” ë¶„í¬: {Counter(y_balanced)}")

# 2. ê· í˜• ë°ì´í„°ì…‹ì„ ìµœì¢… í›ˆë ¨ì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë¶„ë¦¬ (80:20)
X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(
    X_balanced, y_balanced,
    test_size=0.2,
    random_state=SEED,
    stratify=y_balanced
)

# 3. í›ˆë ¨ìš© Datasetì€ ì¦ê°•(ì¢Œìš° ë°˜ì „ í¬í•¨)ì„ ì ìš©, ê²€ì¦ìš©ì€ ë¯¸ì ìš©
final_train_dataset = GestureDataset(X_train_final, y_train_final, config.SEQUENCE_LENGTH, apply_augmentations=True)
final_val_dataset = GestureDataset(X_val_final, y_val_final, config.SEQUENCE_LENGTH, apply_augmentations=False)

final_train_loader = DataLoader(final_train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
final_val_loader = DataLoader(final_val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)

print(f"ìµœì¢… í›ˆë ¨ ë°ì´í„°: {len(final_train_dataset)}ê°œ, ìµœì¢… ê²€ì¦ ë°ì´í„°: {len(final_val_dataset)}ê°œ")

# ##############################################################################
# @title ì…€ 11 (í†µí•© ëª¨ë¸ìš©): ğŸš€ 3ë‹¨ê³„ - ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
# ##############################################################################
print("--- 3ë‹¨ê³„: ìµœì¢… í†µí•© ëª¨ë¸(Final Unified Model) í›ˆë ¨ ì‹¤í–‰ ---")

# 1. ìµœì¢… ëª¨ë¸ì„ ìƒˆë¡­ê²Œ ì´ˆê¸°í™”
final_model = TCN(
    input_size=config.TCN_INPUT_CHANNELS,
    output_size=2,
    num_channels=config.TCN_NUM_CHANNELS,
    kernel_size=config.TCN_KERNEL_SIZE,
    dropout=config.DROPOUT
).to(device)

# 2. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(final_model.parameters(), lr=config.LEARNING_RATE)

print("\nìµœì¢… ëª¨ë¸ ì•„í‚¤í…ì²˜:")
print(final_model)
print(f"\nì´ {config.FINAL_EPOCHS} ì—í¬í¬ ë™ì•ˆ ìµœì¢… ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# 3. í›ˆë ¨ ë£¨í”„ ì‹¤í–‰
best_f1_score = 0.0
FINAL_MODEL_PATH = 'final_model_unified.pt'

for epoch in range(config.FINAL_EPOCHS):
    train_loss = train_one_epoch(final_model, final_train_loader, criterion, optimizer, device)
    val_metrics = evaluate(final_model, final_val_loader, criterion, device)

    print(
        f"Epoch {epoch+1}/{config.FINAL_EPOCHS} | "
        f"Train Loss: {train_loss:.4f} | "
        f"Val F1: {val_metrics['f1_score']:.4f} | "
        f"Val Acc: {val_metrics['accuracy']:.4f}"
    )

    # ê²€ì¦ F1-scoreê°€ ê°€ì¥ ë†’ì€ ëª¨ë¸ì„ ìµœì¢… ëª¨ë¸ë¡œ ì €ì¥
    if val_metrics['f1_score'] > best_f1_score:
        best_f1_score = val_metrics['f1_score']
        torch.save(final_model.state_dict(), FINAL_MODEL_PATH)
        print(f"  -> ğŸ‰ New best model saved with F1-Score: {best_f1_score:.4f}")

print("-" * 50)
print(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… í†µí•© ëª¨ë¸ì´ '{FINAL_MODEL_PATH}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ìµœê³  ê²€ì¦ F1-Score: {best_f1_score:.4f}")

"""# ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: ì…€ 1ì˜ Config í´ë˜ìŠ¤ì— ìˆëŠ” ê°’ë“¤(í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, TCN ì±„ë„ ìˆ˜, ì»¤ë„ í¬ê¸°, ë“œë¡­ì•„ì›ƒ ë“±)ì„ ë³´ê³ ì„œì˜ ê°€ì´ë“œì— ë”°ë¼ ë³€ê²½í•˜ë©° ì…€ 6ë¶€í„° ì…€ 11ê¹Œì§€ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ ë³€í™”ë¥¼ í™•ì¸í•˜ê³  ìµœì ì˜ ì¡°í•©ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. LSTM ëª¨ë¸ê³¼ ë¹„êµ: TCNê³¼ ë™ì¼í•œ 3ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ë‘ ëª¨ë¸ì˜ ìµœì¢… F1-Scoreë¥¼ ë¹„êµí•˜ì—¬ ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì´ íƒœìŠ¤í¬ì— ë” ì í•©í•œì§€ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. ì„±ëŠ¥ ë¶„ì„: evaluate í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall) ê°’ì„ ë¶„ì„í•˜ì—¬, ëª¨ë¸ì´ ì–´ë–¤ í´ë˜ìŠ¤ë¥¼ ë” ì˜ ë§íˆê³  ì–´ë–¤ í´ë˜ìŠ¤ë¥¼ ì–´ë ¤ì›Œí•˜ëŠ”ì§€ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""